Training a deep feed forward network for multidimensional regression.

Assume that you are given a polynomial with multiple (8) inputs and multiple outputs (6) of the form: (ğ‘¦1 , ğ‘¦2 , â€¦ , ğ‘¦6 ) = Î¡(ğ‘¥1 , ğ‘¥2 , â€¦ , ğ‘¥8 ),
where the exact polynomials are:

ğ‘¦1 = ğ‘¥1 ğ‘¥2 ğ‘¥3 + 1. 2ğ‘¥1 ğ‘¥5 âˆ’ 0.1ğ‘¥6 ğ‘¥7 ğ‘¥8 âˆ’ 2ğ‘¥12 ğ‘¥8 + ğ‘¥5
ğ‘¦2 = ğ‘¥1 ğ‘¥5 ğ‘¥6 âˆ’ ğ‘¥3 ğ‘¥4 âˆ’ 3ğ‘¥2 ğ‘¥3 + 2ğ‘¥22 ğ‘¥4 âˆ’ 2ğ‘¥7 ğ‘¥8 âˆ’ 1
ğ‘¦3 = ğ‘¥32 âˆ’ ğ‘¥5 ğ‘¥7 âˆ’ 3ğ‘¥1 ğ‘¥4 ğ‘¥6 âˆ’ ğ‘¥12 ğ‘¥2 ğ‘¥4 âˆ’ 1
ğ‘¦4 = âˆ’ğ‘¥63 + 2ğ‘¥1 ğ‘¥3 ğ‘¥8 âˆ’ ğ‘¥1 ğ‘¥4 ğ‘¥7 âˆ’ 2ğ‘¥52 ğ‘¥2 ğ‘¥4 âˆ’ ğ‘¥8
ğ‘¦5 = ğ‘¥12 ğ‘¥5 âˆ’ 3ğ‘¥3 ğ‘¥4 ğ‘¥8 + ğ‘¥1 ğ‘¥2 ğ‘¥4 âˆ’ 3ğ‘¥6 + ğ‘¥12 ğ‘¥7 + 2
ğ‘¦6 = ğ‘¥12 ğ‘¥3 ğ‘¥6 âˆ’ ğ‘¥3 ğ‘¥5 ğ‘¥7 + ğ‘¥3 ğ‘¥4 + 2.2ğ‘¥4 + ğ‘¥22 ğ‘¥3 âˆ’ 1.1

Model a FFNN such that: (ğ‘¦1 , ğ‘¦2 , â€¦ , ğ‘¦6 ) = FFNN(ğ‘¥1 , ğ‘¥2 , â€¦ , ğ‘¥8 )

Training and validation data:
1.Given the polynomials, generate ğ‘ğ‘¡ instances of data pairs of the form: {(ğ‘¥1ğ‘¡ , ğ‘¥2ğ‘¡ , â€¦ , ğ‘¥8ğ‘¡ ), (ğ‘¦1ğ‘¡ , ğ‘¦2ğ‘¡ , â€¦ , ğ‘¦6ğ‘¡ )}.
2.When needed, add some noise to the training data from a normal distribution with mean ğœ‡ and standard deviation ğœ such that the training data becomes: {(ğ‘¥1ğ‘¡ , ğ‘¥2ğ‘¡ , â€¦ , ğ‘¥8ğ‘¡ ), (ğ‘¦1ğ‘¡ + ğ‘(ğœ‡, ğœ), ğ‘¦2ğ‘¡ + ğ‘(ğœ‡, ğœ), â€¦ , ğ‘¦6ğ‘¡ + ğ‘(ğœ‡, ğœ))}.
3.Similarly, create an additional ğ‘ğ‘£ instances {(ğ‘¥1ğ‘£ , ğ‘¥2ğ‘£ , â€¦ , ğ‘¥8ğ‘£ ), (ğ‘¦1ğ‘£ , ğ‘¦2ğ‘£ , â€¦ , ğ‘¦6ğ‘£ )} for validation of the trained model.

Model training:
1.Choose ğ‘ğ‘¡ to be 1000.
2.In your training data add some noise to ğ‘¦ğ‘– â€™s from a normal distribution with ğœ‡ = 0.0 and ğœ = 0.001.
3.Build a feed forward network with exactly 3 hidden layers:
    a.Each layer should include exactly 6 nodes in the beginning.
    b.Use a combination of activation functions in these layers (use the same activation for each node at a given layer).
4.Define your loss function:
    a.Use MSE for loss function.
5.Train your algorithm with SGD.
    a.Use appropriate learning rates and the number of epochs.
    b.Report the training and validation errors.
6.Repeat Steps 2-4 with another set of activation functions (3 different combinations), learning rates (3 different schemes) and number of epochs (after finding a reasonable number of epochs in the first trial, increase by 50% for 2 times).
7.Choose your best parameters after Step
8.Add new nodes at a time to each hidden layer:
    a.Start from the first hidden layer, add two nodes, train, and record results.
    b.Move to the second hidden layer, add two nodes, train, and record results.
    c.Move to the third hidden layer, add two nodes, train, and record results.
    d.Repeat Step 8 until bias and variance curve is drawn (see Figure 1 for a fictitious example from the first lecture).
9.Increase ğ‘ğ‘¡ by 10% and repeat Step 8.
10.Report your all results.